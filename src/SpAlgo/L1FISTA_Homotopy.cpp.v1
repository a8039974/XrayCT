#include "SpAlgo.hpp"
#include "BlobProjector.hpp"
// L1 minimization by Homotopy continuation with FISTA

namespace SpAlgo {
  ConvMsg L1FISTA_Homotopy(LinOp &A, const ArrayXd &W0, const ArrayXd &Y, ArrayXd &X,
			   double mu, double tol, int maxIter, double stol, 
			   int hIter, double incr, int debias, int verbose)
  {
    // A : system operator
    // Y : data in vector form
    // W : weight for L1 norm
    // X : initialization in vector form, output is rewritten on X
    // mu : L1 fidelity penalty, the most important argument, should be set according to noise level, nbProj, pixDet, sizeObj, and nbNode
    // tol : iteration tolerance
    // maxIter : maximum iteration steps
    // verbose : display convergence message

    if (verbose) {
      cout<<"-----L1 minimization by FISTA method with homotopy continuation of parameter mu-----"<<endl;
      cout<<"Parameters :"<<endl;
      cout<<"Objective L1 fidelity penalty : "<<mu<<endl;
      cout<<"FISTA max. iterations : "<<maxIter<<endl;
      cout<<"FISTA stopping tolerance : "<<tol<<endl;
      cout<<"FISTA support chg. stopping tolerance : "<<tol<<endl;
      cout<<"Iteration number of homotopy continuation : "<<hIter<<endl;
      cout<<"Homotopy incremental factor : "<<incr<<endl;
      cout<<"CG debiasing max. iterations : "<<debias<<endl;
    }

    BlobProjector *P = dynamic_cast<BlobProjector *>(const_cast<LinOp *>(&A));
    // Used only by general projector
    DiagOp M(X.size(), 1.);
    CompOp *AM = new CompOp(&A, &M);
    AtAOp L(AM);

    ConvMsg* msg = new ConvMsg[hIter];
    double mu0 = mu/pow(incr, hIter-1);
    ArrayXd W = W0;
    //int maxIter0 = int(AP.maxIter/AP.cont);
    //double stol0 = AP.stol/pow(AP.incr, AP.cont-1);
    //double mu = AP.mu*W.abs().sum();
    // cout<<maxIter0<<endl;
    // cout<<mu0<<endl;
    for(int n=0; n<hIter; n++) {
      if (verbose and hIter>1)
	cout<<"\nHomotopy continuation sub-iteration : "<<n<<endl;    	
      msg[n] = L1FISTA(*P, W, Y, X, mu0*X.size()/W.sum(), tol, maxIter, stol, verbose-1);
      mu0 *= incr;

      // Make RW mask
      W = X.abs();
      double gmax = W.maxCoeff();
      for (size_t m=0; m<W.size(); m++)
	W[m] = 1/(W[m] + 0.001 * gmax);
      W /= W.maxCoeff();

      if (verbose) {
	int nL0 = Tools::l0norm(X);
	//printf("Niter = %d\tvobj = %1.5e\t|Ax-b| = %1.5e\tL1 norm = %1.5e\tnon zero per. = %1.5e\n", msg[n].niter, msg[n].vobj, msg[n].res, msg[n].norm, nL0*1./X.size());
	printf("Niter = %d\t|Ax-b| = %1.5e\tL1 norm = %1.5e\tnon zero per. = %1.5e\n", msg[n].niter, msg[n].res, msg[n].norm, nL0*1./X.size());
      }

      if (debias>0) {
	if (verbose) {
	  cout<<"Debiasing by Conjugate Gradient method..."<<endl;  
	  cout<<"Residual |Ax-b| before debiasing = "<<(A.forward(X)-Y).matrix().norm()<<endl;
	}

	if (P==NULL) {
	  ArrayXd S = (X.abs()>0).select(1, ArrayXd::Zero(X.size()));
	  M.set_a(S);
	  //LinSolver::Solver_normal_CG(A, Y, X, 100, 1e-2, &MSupp, verbose);
	  LinSolver::Solver_CG(L, AM->backward(Y), X, debias, 1e-3, false);
	}
	else {
	  vector<ArrayXd> XS = P->separate(X);
	  vector <ArrayXb> S; S.resize(XS.size());
	  for (int n=0; n<XS.size(); n++) {
	    S[n] = (XS[n].abs()>0).select(true, ArrayXb::Constant(XS[n].size(), false));
	  }
	  P->set_maskBlob(S);
	  AtAOp L(P);
	  LinSolver::Solver_CG(L, P->backward(Y), X, debias, 1e-3, false);
	  P->reset_maskBlob();
	}
	
	if (verbose) {	
	  cout<<"Residual |Ax-b| after debiasing = "<<(A.forward(X)-Y).matrix().norm()<<endl;
	}
      }
    }

    double nL1 = X.abs().sum();
    double res = (A.forward(X)-Y).matrix().norm();
    double vobj = 0.5 * res * res + mu * nL1;
    int niter;
    for (int n=0; n<hIter; n++)
      niter += msg[n].niter;

    return ConvMsg(niter, vobj, res, nL1);
  }
}


// The following is the IST algorithm (slower)

    // while (niter < maxIter and RdX > tol) {    
    //   A.backward(AX-Y, gradX);

    //   if (niter == 0) { // Do steepest descent at 1st iteration
    // 	// Gradient of AL function      
    // 	ArrayXd AgradX = A.forward(gradX);
    // 	gradstep = (gradX*gradX).sum() / (AgradX*AgradX).sum();
    //   }
    //   else {
    // 	// Set gradstep through BB formula
    // 	dgradX = gradX - gradX0;
    // 	gradstep = (dX * dX).sum() / (dX * dgradX).sum();
    //   }
      
    //   X = l1_shrink(X - gradstep * gradX, gradstep*mu, W);

    //   A.forward(X, AX);  
    //   dX = X - X0;
    //   X0 = X;
    //   gradX0 = gradX;

    //   RdX = (niter==0) ? 1. : (dX).matrix().norm() / X0.matrix().norm();
    // }
